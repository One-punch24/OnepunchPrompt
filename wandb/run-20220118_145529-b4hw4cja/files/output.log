
{'state': {}, 'param_groups': [{'weight_decay': 0.0, 'lr': 5e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'correct_bias': True, 'params': [0, 1, 2]}, {'weight_decay': 0.0, 'lr': 5e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'correct_bias': True, 'params': [3, 4]}]}
Traceback (most recent call last):
  File "/home/onepunch/OnePunch/OnepunchPrompt/Prefix_tune.py", line 138, in <module>
    Prefix_trainer.train()
  File "/home/onepunch/OnePunch/OnepunchPrompt/Prefixtrainer.py", line 364, in train
    out,loss = self.model(**input_)
  File "/home/onepunch/anaconda3/envs/onepunch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/onepunch/OnePunch/OnepunchPrompt/prompt_models/Prefix_model.py", line 225, in forward
    return_dict = return_dict)
  File "/home/onepunch/anaconda3/envs/onepunch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/onepunch/OnePunch/OnepunchPrompt/transformers/src/transformers/modeling_gpt2.py", line 952, in forward
    return_dict=return_dict,
  File "/home/onepunch/anaconda3/envs/onepunch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/onepunch/OnePunch/OnepunchPrompt/transformers/src/transformers/modeling_gpt2.py", line 645, in forward
    output_attentions=output_attentions,
  File "/home/onepunch/anaconda3/envs/onepunch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/onepunch/OnePunch/OnepunchPrompt/transformers/src/transformers/modeling_gpt2.py", line 302, in forward
    output_attentions=output_attentions,
  File "/home/onepunch/anaconda3/envs/onepunch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/onepunch/OnePunch/OnepunchPrompt/transformers/src/transformers/modeling_gpt2.py", line 246, in forward
    attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)
  File "/home/onepunch/OnePunch/OnepunchPrompt/transformers/src/transformers/modeling_gpt2.py", line 163, in _attn
    w = torch.matmul(q, k)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 2.45 GiB already allocated; 3.19 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF